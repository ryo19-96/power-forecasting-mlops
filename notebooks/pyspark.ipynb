{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f8306-aaa3-4c0c-9d05-2b2bd4ca089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FeatureEngineering\").master(\"local[*]\").config(\"spark.submit.pyFiles\", \"./holidays_package.zip\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8bdec-a541-440d-ab73-846e365af2ca",
   "metadata": {},
   "source": [
    "# Data_LoaderのPySpark検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3a9a46-48ff-410e-a6b5-706da33a7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ZIPファイルを展開\n",
    "zip_dir = \"../data/power_usage\"\n",
    "extract_dir = \"../data/power_extracted\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "for zip_name in sorted(os.listdir(zip_dir)):\n",
    "    if zip_name.endswith(\".zip\"):\n",
    "        zip_path = os.path.join(zip_dir, zip_name)\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e959ab-c50f-4e4e-87ba-5d60ca3046f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.types import IntegerType, LongType, StringType, StructField, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828a9592-5bee-4b8d-96b1-f0d721a317cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_extract_max_power(csv_path):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, encoding=\"shift-jis\", skiprows=54)\n",
    "        max_power = int(df[\"当日実績(５分間隔値)(万kW)\"].max())\n",
    "        date = os.path.basename(csv_path).split(\"_\")[0]  # YYYYMMDDを取得\n",
    "        return {\"date\": date, \"max_power\": max_power}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {csv_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# pandasで抽出し、Spark DataFrame化\n",
    "records = []\n",
    "for fname in os.listdir(extract_dir):\n",
    "    if fname.endswith(\".csv\"):\n",
    "        path = os.path.join(extract_dir, fname)\n",
    "        record = read_and_extract_max_power(path)\n",
    "        if record:\n",
    "            records.append(record)\n",
    "\n",
    "# pandas → Spark DataFrame\n",
    "power_usage_df = spark.createDataFrame(records, StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"max_power\", LongType(), True),\n",
    "]))\n",
    "\n",
    "# 日付形式に変換\n",
    "power_usage_df = power_usage_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyyMMdd\"))\n",
    "\n",
    "# 表示確認\n",
    "power_usage_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0beb7d6-2ab8-45f1-a443-d8fa98764a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weather_data(path) -> pd.DataFrame:\n",
    "        \"\"\"気象データファイルを読み込む\n",
    "        pysparkだとskiprowが設定できないため一度pandas dataframeで読み込む\n",
    "\n",
    "        Args:\n",
    "            encoding: ファイルエンコーディング\n",
    "            skiprows: スキップする行番号のリスト\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 気象データフレーム\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(path, encoding=\"shift-jis\", skiprows=[0, 1, 2, 4, 5])\n",
    "\n",
    "        # 必要なカラムだけ抽出\n",
    "        df = df[[\"年月日\", \"最高気温(℃)\", \"最低気温(℃)\", \"天気概況(昼：06時〜18時)\"]]\n",
    "\n",
    "        # カラム名を英語に変更\n",
    "        df = df.rename(\n",
    "            columns={\n",
    "                \"年月日\": \"date\",\n",
    "                \"最高気温(℃)\": \"max_temp\",\n",
    "                \"最低気温(℃)\": \"min_temp\",\n",
    "                \"天気概況(昼：06時〜18時)\": \"weather\",\n",
    "            },\n",
    "        )# 日付をdatetime型に変換\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y/%m/%d\")\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa83cbc-376d-4b97-be3f-4ed1e9ad2ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/weather_data.csv\"\n",
    "df = load_weather_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a13643-7e6f-4404-be83-04f511c3f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = spark.createDataFrame(df)\n",
    "# timestampからdateにするためSparkに渡したあとに明示的に日付だけに変換\n",
    "weather_df = weather_df.withColumn(\"date\", to_date(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312efd41-3fb5-4e0f-a865-8fcca8110eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ebfa01-d258-435a-a5ce-67302f30f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの結合\n",
    "weather_df = weather_df.repartition(60, \"date\")\n",
    "power_usage_df = power_usage_df.repartition(60, \"date\")\n",
    "\n",
    "merge_data = weather_df.join(\n",
    "    power_usage_df,\n",
    "    on = [\"date\"],\n",
    "    how=\"inner\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac620b-0b55-4141-85d5-432592297a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a9613f-27a8-4656-819b-79dce571941e",
   "metadata": {},
   "source": [
    "# 前処理、特徴量エンジニアリングの確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf73efe-dbf3-4c7a-9358-07e88c39eeb1",
   "metadata": {},
   "source": [
    "### 天気のカテゴリ変数まとめ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc156463-17b2-4e5c-8e06-c08976159658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def categorize_weather(weather):\n",
    "    if weather is None:\n",
    "        return \"不明\"\n",
    "    if \"雪\" in weather or \"ゆき\" in weather:\n",
    "        return \"雪\"\n",
    "    if \"雷\" in weather:\n",
    "        if \"雨\" in weather or \"あめ\" in weather:\n",
    "            return \"雷雨\"\n",
    "        if \"晴\" in weather:\n",
    "            return \"晴れ(雷あり)\"\n",
    "        if \"曇\" in weather:\n",
    "            return \"曇り(雷あり)\"\n",
    "        return \"雷\"\n",
    "    if \"快晴\" in weather:\n",
    "        return \"快晴\"\n",
    "    if \"晴\" in weather:\n",
    "        if \"曇\" in weather:\n",
    "            return \"晴れ時々曇り\"\n",
    "        if \"雨\" in weather or \"あめ\" in weather or \"雷\" in weather:\n",
    "            return \"晴れ時々雨\"\n",
    "        return \"晴れ\"\n",
    "    if \"曇\" in weather:\n",
    "        if \"雨\" in weather or \"あめ\" in weather:\n",
    "            return \"曇り時々雨\"\n",
    "        return \"曇り\"\n",
    "    if \"雨\" in weather or \"あめ\" in weather:\n",
    "        return \"雨\"\n",
    "    return \"その他\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b881ee-489e-4c93-bd38-18cffa01a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_weather_spark(df, weather_col=\"weather\"):\n",
    "    return df.withColumn(\"weather_category\", categorize_weather(df[weather_col])).drop(weather_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5716458b-485b-4725-a03a-f083638b1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = categorize_weather_spark(merge_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e3e25-b2ad-4ae2-8ba3-bbeb7b11dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26902535-3211-4e1c-b882-07658c5efb43",
   "metadata": {},
   "source": [
    "### 数値系特徴量作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a4abc-00be-47c7-b467-87a7f09cfb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numeric_features_spark(df):\n",
    "    avg = (col(\"max_temp\") + col(\"min_temp\")) / 2\n",
    "    rng = col(\"max_temp\") - col(\"min_temp\")\n",
    "    cdd = (avg - 18).cast(\"double\")\n",
    "    hdd = (18 - avg).cast(\"double\")\n",
    "\n",
    "    return (\n",
    "        df.withColumn(\"avg\", avg)\n",
    "          .withColumn(\"rng\", rng)\n",
    "          .withColumn(\"cdd\", cdd)\n",
    "          .withColumn(\"hdd\", hdd)\n",
    "          .withColumn(\"hot\", (col(\"max_temp\") >= 30))\n",
    "          .withColumn(\"cold\", (col(\"min_temp\") <= 5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c1bd6f-f41c-4905-b982-bf48915fa320",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = create_numeric_features_spark(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f007269-45d3-45a2-a7c1-d0a01aa85574",
   "metadata": {},
   "source": [
    "### カレンダー系特徴量作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6768be00-7743-411e-9f39-d93fc72a86ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"holiday-udf\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .config(\"spark.submit.pyFiles\", \"../holidays_package.zip\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af583f63-b3e3-40ee-9023-9a67dcb4217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import cos, dayofmonth, dayofweek, month, sin, year\n",
    "\n",
    "jp_holidays = holidays.Japan()\n",
    "\n",
    "# 祝日判定\n",
    "@udf(returnType=IntegerType())\n",
    "def is_holiday(date):\n",
    "    return int(date in jp_holidays)\n",
    "\n",
    "def create_calendar_features_spark(df, date_col=\"date\"):\n",
    "    return (\n",
    "        df.withColumn(\"year\", year(col(date_col)))\n",
    "          .withColumn(\"month\", month(col(date_col)))\n",
    "          .withColumn(\"day\", dayofmonth(col(date_col)))\n",
    "        # 1=日曜〜7=土曜\n",
    "          .withColumn(\"dow\", dayofweek(col(date_col)))\n",
    "          .withColumn(\"dow_sin\", sin(2 * np.pi * (col(\"dow\") - 1) / 7))\n",
    "          .withColumn(\"dow_cos\", cos(2 * np.pi * (col(\"dow\") - 1) / 7))\n",
    "          .withColumn(\"mon_sin\", sin(2 * np.pi * col(\"month\") / 12))\n",
    "          .withColumn(\"mon_cos\", cos(2 * np.pi * col(\"month\") / 12))\n",
    "          .withColumn(\"weekend\", ((col(\"dow\") == 1) | (col(\"dow\") == 7)).cast(\"int\"))\n",
    "          .withColumn(\"holiday\", is_holiday(col(date_col)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba982df-6205-435d-a1f3-3dbe09c8a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_calendar_features_spark(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5f372-7f7d-484e-986d-4aa1ed11c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e357e6df-ce30-401f-8119-c562eb3499cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"2023-01-01\"\n",
    "yyyymm = \"-\".join(a.split(\"-\")[:2]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4caa60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"~/downloads/part-00000-44afcced-ef61-4c52-a52f-38f13c083579.c000.snappy.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f734ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04487a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power-forecasting-mlops",
   "language": "python",
   "name": "power-forecasting-mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
