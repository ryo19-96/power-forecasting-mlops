{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c7f8306-aaa3-4c0c-9d05-2b2bd4ca089b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/28 09:16:06 WARN Utils: Your hostname, watanaberyounoMacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.16.164 instead (on interface en0)\n",
      "25/05/28 09:16:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/05/28 09:16:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 65095)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ryo/.pyenv/versions/3.10.8/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/Users/ryo/.pyenv/versions/3.10.8/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/Users/ryo/.pyenv/versions/3.10.8/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/Users/ryo/.pyenv/versions/3.10.8/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/Users/ryo/personal_project/power-forecasting-mlops/venv/lib/python3.10/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/Users/ryo/personal_project/power-forecasting-mlops/venv/lib/python3.10/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/Users/ryo/personal_project/power-forecasting-mlops/venv/lib/python3.10/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/Users/ryo/personal_project/power-forecasting-mlops/venv/lib/python3.10/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FeatureEngineering\").master(\"local[*]\").config(\"spark.submit.pyFiles\", \"../holidays_package.zip\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f8bdec-a541-440d-ab73-846e365af2ca",
   "metadata": {},
   "source": [
    "# Data_LoaderのPySpark検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3a9a46-48ff-410e-a6b5-706da33a7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ZIPファイルを展開\n",
    "zip_dir = \"../data/power_usage\"\n",
    "extract_dir = \"../data/power_extracted\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "for zip_name in sorted(os.listdir(zip_dir)):\n",
    "    if zip_name.endswith(\".zip\"):\n",
    "        zip_path = os.path.join(zip_dir, zip_name)\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60e959ab-c50f-4e4e-87ba-5d60ca3046f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.types import IntegerType, LongType, StringType, StructField, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "828a9592-5bee-4b8d-96b1-f0d721a317cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|      date|max_power|\n",
      "+----------+---------+\n",
      "|2023-02-19|     3216|\n",
      "|2022-10-03|     3813|\n",
      "|2023-11-12|     3160|\n",
      "|2023-02-24|     4110|\n",
      "|2024-02-07|     4791|\n",
      "+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_and_extract_max_power(csv_path):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, encoding=\"shift-jis\", skiprows=54)\n",
    "        max_power = int(df[\"当日実績(５分間隔値)(万kW)\"].max())\n",
    "        date = os.path.basename(csv_path).split(\"_\")[0]  # YYYYMMDDを取得\n",
    "        return {\"date\": date, \"max_power\": max_power}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {csv_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# pandasで抽出し、Spark DataFrame化\n",
    "records = []\n",
    "for fname in os.listdir(extract_dir):\n",
    "    if fname.endswith(\".csv\"):\n",
    "        path = os.path.join(extract_dir, fname)\n",
    "        record = read_and_extract_max_power(path)\n",
    "        if record:\n",
    "            records.append(record)\n",
    "\n",
    "# pandas → Spark DataFrame\n",
    "power_usage_df = spark.createDataFrame(records, StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"max_power\", LongType(), True),\n",
    "]))\n",
    "\n",
    "# 日付形式に変換\n",
    "power_usage_df = power_usage_df.withColumn(\"date\", to_date(col(\"date\"), \"yyyyMMdd\"))\n",
    "\n",
    "# 表示確認\n",
    "power_usage_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0beb7d6-2ab8-45f1-a443-d8fa98764a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weather_data(path) -> pd.DataFrame:\n",
    "        \"\"\"気象データファイルを読み込む\n",
    "        pysparkだとskiprowが設定できないため一度pandas dataframeで読み込む\n",
    "\n",
    "        Args:\n",
    "            encoding: ファイルエンコーディング\n",
    "            skiprows: スキップする行番号のリスト\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: 気象データフレーム\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(path, encoding=\"shift-jis\", skiprows=[0, 1, 2, 4, 5])\n",
    "\n",
    "        # 必要なカラムだけ抽出\n",
    "        df = df[[\"年月日\", \"最高気温(℃)\", \"最低気温(℃)\", \"天気概況(昼：06時〜18時)\"]]\n",
    "\n",
    "        # カラム名を英語に変更\n",
    "        df = df.rename(\n",
    "            columns={\n",
    "                \"年月日\": \"date\",\n",
    "                \"最高気温(℃)\": \"max_temp\",\n",
    "                \"最低気温(℃)\": \"min_temp\",\n",
    "                \"天気概況(昼：06時〜18時)\": \"weather\",\n",
    "            },\n",
    "        )# 日付をdatetime型に変換\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], format=\"%Y/%m/%d\")\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afa83cbc-376d-4b97-be3f-4ed1e9ad2ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/weather_data.csv\"\n",
    "df = load_weather_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65a13643-7e6f-4404-be83-04f511c3f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = spark.createDataFrame(df)\n",
    "# timestampからdateにするためSparkに渡したあとに明示的に日付だけに変換\n",
    "weather_df = weather_df.withColumn(\"date\", to_date(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "312efd41-3fb5-4e0f-a865-8fcca8110eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+--------+\n",
      "|      date|max_temp|min_temp| weather|\n",
      "+----------+--------+--------+--------+\n",
      "|2018-01-01|    13.0|     0.4|      晴|\n",
      "|2018-01-02|    10.8|     0.8|    快晴|\n",
      "|2018-01-03|     8.6|     2.3|    快晴|\n",
      "|2018-01-04|     9.6|     0.0|    快晴|\n",
      "|2018-01-05|     6.3|     0.8|曇一時雨|\n",
      "+----------+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82ebfa01-d258-435a-a5ce-67302f30f190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの結合\n",
    "weather_df = weather_df.repartition(60, \"date\")\n",
    "power_usage_df = power_usage_df.repartition(60, \"date\")\n",
    "\n",
    "merge_data = weather_df.join(\n",
    "    power_usage_df,\n",
    "    on = [\"date\"],\n",
    "    how=\"inner\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98ac620b-0b55-4141-85d5-432592297a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_data.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a9613f-27a8-4656-819b-79dce571941e",
   "metadata": {},
   "source": [
    "# 前処理、特徴量エンジニアリングの確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf73efe-dbf3-4c7a-9358-07e88c39eeb1",
   "metadata": {},
   "source": [
    "### 天気のカテゴリ変数まとめ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc156463-17b2-4e5c-8e06-c08976159658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def categorize_weather(weather):\n",
    "    if weather is None:\n",
    "        return \"不明\"\n",
    "    if \"雪\" in weather or \"ゆき\" in weather:\n",
    "        return \"雪\"\n",
    "    if \"雷\" in weather:\n",
    "        if \"雨\" in weather or \"あめ\" in weather:\n",
    "            return \"雷雨\"\n",
    "        if \"晴\" in weather:\n",
    "            return \"晴れ(雷あり)\"\n",
    "        if \"曇\" in weather:\n",
    "            return \"曇り(雷あり)\"\n",
    "        return \"雷\"\n",
    "    if \"快晴\" in weather:\n",
    "        return \"快晴\"\n",
    "    if \"晴\" in weather:\n",
    "        if \"曇\" in weather:\n",
    "            return \"晴れ時々曇り\"\n",
    "        if \"雨\" in weather or \"あめ\" in weather or \"雷\" in weather:\n",
    "            return \"晴れ時々雨\"\n",
    "        return \"晴れ\"\n",
    "    if \"曇\" in weather:\n",
    "        if \"雨\" in weather or \"あめ\" in weather:\n",
    "            return \"曇り時々雨\"\n",
    "        return \"曇り\"\n",
    "    if \"雨\" in weather or \"あめ\" in weather:\n",
    "        return \"雨\"\n",
    "    return \"その他\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89b881ee-489e-4c93-bd38-18cffa01a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_weather_spark(df, weather_col=\"weather\"):\n",
    "    return df.withColumn(\"weather_category\", categorize_weather(df[weather_col])).drop(weather_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5716458b-485b-4725-a03a-f083638b1933",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = categorize_weather_spark(merge_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd7e3e25-b2ad-4ae2-8ba3-bbeb7b11dc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+---------+----------------+\n",
      "|      date|max_temp|min_temp|max_power|weather_category|\n",
      "+----------+--------+--------+---------+----------------+\n",
      "|2022-07-31|    35.0|    27.2|     4691|            晴れ|\n",
      "|2023-06-22|    22.6|    16.7|     3456|      曇り時々雨|\n",
      "|2023-07-15|    32.9|    26.5|     3861|            曇り|\n",
      "|2024-09-18|    35.1|    24.8|     5424|    晴れ時々曇り|\n",
      "|2022-07-27|    33.6|    26.2|     5256|    晴れ時々曇り|\n",
      "|2022-08-02|    35.9|    27.6|     5952|    晴れ時々曇り|\n",
      "|2022-11-29|    21.3|    11.6|     3572|      曇り時々雨|\n",
      "|2022-12-25|    13.1|     1.8|     3741|            晴れ|\n",
      "|2023-05-22|    29.4|    16.5|     3717|    晴れ時々曇り|\n",
      "|2023-09-14|    32.7|    25.1|     5036|            晴れ|\n",
      "|2023-11-08|    21.6|    14.6|     3223|            快晴|\n",
      "|2024-02-05|     6.2|     0.3|     5007|              雪|\n",
      "|2024-05-30|    27.3|    17.5|     3542|    晴れ時々曇り|\n",
      "|2024-06-04|    25.5|    14.8|     3470|            晴れ|\n",
      "|2024-06-12|    30.1|    20.0|     4041|    晴れ時々曇り|\n",
      "|2024-08-27|    31.5|    24.5|     4853|      曇り時々雨|\n",
      "|2022-05-19|    26.6|    14.2|     3547|    晴れ時々曇り|\n",
      "|2023-02-25|    12.7|     4.0|     3708|    晴れ時々曇り|\n",
      "|2023-06-18|    31.1|    21.2|     3278|    晴れ時々曇り|\n",
      "|2023-09-19|    33.5|    25.1|     5172|            晴れ|\n",
      "+----------+--------+--------+---------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26902535-3211-4e1c-b882-07658c5efb43",
   "metadata": {},
   "source": [
    "### 数値系特徴量作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd7a4abc-00be-47c7-b467-87a7f09cfb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_numeric_features_spark(df):\n",
    "    avg = (col(\"max_temp\") + col(\"min_temp\")) / 2\n",
    "    rng = col(\"max_temp\") - col(\"min_temp\")\n",
    "    cdd = (avg - 18).cast(\"double\")\n",
    "    hdd = (18 - avg).cast(\"double\")\n",
    "\n",
    "    return (\n",
    "        df.withColumn(\"avg\", avg)\n",
    "          .withColumn(\"rng\", rng)\n",
    "          .withColumn(\"cdd\", cdd)\n",
    "          .withColumn(\"hdd\", hdd)\n",
    "          .withColumn(\"hot\", (col(\"max_temp\") >= 30))\n",
    "          .withColumn(\"cold\", (col(\"min_temp\") <= 5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2c1bd6f-f41c-4905-b982-bf48915fa320",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = create_numeric_features_spark(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f007269-45d3-45a2-a7c1-d0a01aa85574",
   "metadata": {},
   "source": [
    "### カレンダー系特徴量作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6768be00-7743-411e-9f39-d93fc72a86ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"holiday-udf\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .config(\"spark.submit.pyFiles\", \"../holidays_package.zip\") \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af583f63-b3e3-40ee-9023-9a67dcb4217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import cos, dayofmonth, dayofweek, month, sin, year\n",
    "\n",
    "jp_holidays = holidays.Japan()\n",
    "\n",
    "# 祝日判定\n",
    "@udf(returnType=IntegerType())\n",
    "def is_holiday(date):\n",
    "    return int(date in jp_holidays)\n",
    "\n",
    "def create_calendar_features_spark(df, date_col=\"date\"):\n",
    "    return (\n",
    "        df.withColumn(\"year\", year(col(date_col)))\n",
    "          .withColumn(\"month\", month(col(date_col)))\n",
    "          .withColumn(\"day\", dayofmonth(col(date_col)))\n",
    "        # 1=日曜〜7=土曜\n",
    "          .withColumn(\"dow\", dayofweek(col(date_col)))\n",
    "          .withColumn(\"dow_sin\", sin(2 * np.pi * (col(\"dow\") - 1) / 7))\n",
    "          .withColumn(\"dow_cos\", cos(2 * np.pi * (col(\"dow\") - 1) / 7))\n",
    "          .withColumn(\"mon_sin\", sin(2 * np.pi * col(\"month\") / 12))\n",
    "          .withColumn(\"mon_cos\", cos(2 * np.pi * col(\"month\") / 12))\n",
    "          .withColumn(\"weekend\", ((col(\"dow\") == 1) | (col(\"dow\") == 7)).cast(\"int\"))\n",
    "          .withColumn(\"holiday\", is_holiday(col(date_col)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ba982df-6205-435d-a1f3-3dbe09c8a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_calendar_features_spark(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57b5f372-7f7d-484e-986d-4aa1ed11c9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+--------+---------+----------------+------------------+-----------------+------------------+-------------------+-----+-----+----+-----+---+---+-------------------+-------------------+--------------------+--------------------+-------+-------+\n",
      "|      date|max_temp|min_temp|max_power|weather_category|               avg|              rng|               cdd|                hdd|  hot| cold|year|month|day|dow|            dow_sin|            dow_cos|             mon_sin|             mon_cos|weekend|holiday|\n",
      "+----------+--------+--------+---------+----------------+------------------+-----------------+------------------+-------------------+-----+-----+----+-----+---+---+-------------------+-------------------+--------------------+--------------------+-------+-------+\n",
      "|2022-07-31|    35.0|    27.2|     4691|            晴れ|              31.1|7.800000000000001|13.100000000000001|-13.100000000000001| true|false|2022|    7| 31|  1|                0.0|                1.0| -0.4999999999999997| -0.8660254037844388|      1|      0|\n",
      "|2023-06-22|    22.6|    16.7|     3456|      曇り時々雨|             19.65|5.900000000000002|1.6499999999999986|-1.6499999999999986|false|false|2023|    6| 22|  5| -0.433883739117558|-0.9009688679024191|1.224646799147353...|                -1.0|      0|      0|\n",
      "|2023-07-15|    32.9|    26.5|     3861|            曇り|              29.7|6.399999999999999|              11.7|              -11.7| true|false|2023|    7| 15|  7|-0.7818314824680299| 0.6234898018587334| -0.4999999999999997| -0.8660254037844388|      1|      0|\n",
      "|2024-09-18|    35.1|    24.8|     5424|    晴れ時々曇り|29.950000000000003|             10.3|11.950000000000003|-11.950000000000003| true|false|2024|    9| 18|  4|0.43388373911755823| -0.900968867902419|                -1.0|-1.83697019872102...|      0|      0|\n",
      "|2022-07-27|    33.6|    26.2|     5256|    晴れ時々曇り|              29.9|7.400000000000002|11.899999999999999|-11.899999999999999| true|false|2022|    7| 27|  4|0.43388373911755823| -0.900968867902419| -0.4999999999999997| -0.8660254037844388|      0|      0|\n",
      "+----------+--------+--------+---------+----------------+------------------+-----------------+------------------+-------------------+-----+-----+----+-----+---+---+-------------------+-------------------+--------------------+--------------------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e357e6df-ce30-401f-8119-c562eb3499cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power-forecasting-mlops",
   "language": "python",
   "name": "power-forecasting-mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
